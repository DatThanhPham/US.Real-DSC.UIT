{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('464', 'Biển miền Trung nước đẹp nhỉ', 3), ('7413', 'Chắc là nắc cụt rồi\\n#phetphaikhong', 3), ('3808', 'Nhiều khi ta muốn ta được thiếu nợ\\nĐể khi đi trốn có người đi tìmm', 0), ('5816', 'Phi công này 1 người lái thôi, ai đụng vào là chớt với chuỵ', 3), ('1632', 'Ủy ban Nhân dân thành phố Đà Nẵng vừa có văn bản về việc sẽ không tổ chức phun lửa, phun nước cầu Rồng và không quay cầu sông Hàn trong các đêm trình diễn Lễ hội pháo hoa Quốc tế Đà Nẵng (DIFF) 2023 để nhằm đảm bảo an toàn, hạn chế ùn tắc giao thông tại các cây cầu qua sông và khu vực trung tâm thành phố.\\nCụ thể, không phun nước, phun lửa cầu Rồng trong các đêm 2/6, 10/6, 17/6, 24/6 và 8/7/2023. Không quay cầu sông Hàn trong các đêm 10/6, 17/6, 24/6 và 8/7/2023.', 0)]\n",
      "Train size: 50, Test size: 51\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split    \n",
    "\n",
    "def load_data_from_file(json_file_path, image_fold_path):\n",
    "    with open(json_file_path, 'r') as f: #Open file to read file json\n",
    "        data = json.load(f)\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for entry_id, entry in data.items():\n",
    "        img = Image.open(f'./{image_fold_path}/{entry['image']}')\n",
    "        caption = entry['caption']\n",
    "        if entry['label'] == 'not-sarcasm':\n",
    "            label = 0\n",
    "        elif entry['label'] == 'text-sarcasm':\n",
    "            label = 1\n",
    "        elif entry['label'] == 'image-sarcasm':\n",
    "            label = 2\n",
    "        else:\n",
    "            label = 3\n",
    "        dataset.append((entry_id, caption, label))\n",
    "    return dataset\n",
    "\n",
    "dataset = load_data_from_file('vimmsd-warmup.json', 'warmup-images')\n",
    "print(dataset[:5])\n",
    "\n",
    "# Chia dữ liệu với tỷ lệ 50 50\n",
    "train_data, test_data = train_test_split(dataset, test_size=0.5, random_state=42)\n",
    "\n",
    "print(f'Train size: {len(train_data)}, Test size: {len(test_data)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PREPARE DATA FOR PHOBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader #Dataset để định nghĩa tập dữ liệu, #DataLoader tạo các batch để nạp dữ liệu\n",
    "from transformers import AutoTokenizer #Tải mô hình ngôn ngữ PhoBert\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('vinai/phobert-base', use_fast = False) \n",
    "# biến đổi câu thành token, ko dùng use_fast để đảm bảo có thể dùng phiên bản mặc định\n",
    "\n",
    "class SarcasmTextDataset(Dataset): #Class dùng để quản lí dữ liệu cho việc train \n",
    "    def __init__(self, data, tokenizer, max_len=128):\n",
    "        self.data = data #data bao gom caption va label\n",
    "        self.tokenizer = tokenizer #doi van ban thanh token\n",
    "        self.max_len = max_len #do dai toi da cua cau, dai hon se bi cat, ngan hon duoc dem them\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) #tra ve do dai tap du lieu, so mau du lieu co trong tap du lieu\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        entry_id, caption, label = self.data[idx] # lay caption va label cua data\n",
    "\n",
    "        #tokenize\n",
    "        encoding = self.tokenizer(\n",
    "            caption,\n",
    "            add_special_tokens = True, # them token dac biet [CLS], [SEP]\n",
    "            max_length = self.max_len, # do dai toi da cua chuoi\n",
    "            padding = 'max_length', # them padding neu chuoi ngan hon maxlen\n",
    "            truncation = True, # cat bo chuoi qua dai\n",
    "            return_tensors = 'pt', # tra ve du lieu dang pytorch\n",
    "            clean_up_tokenization_spaces=False  # tắt việc dọn dẹp khoảng trắng trong token\n",
    "        )\n",
    "        return {\n",
    "            'id': entry_id,  # Thêm id vào output\n",
    "            'input_ids':encoding['input_ids'].squeeze(),\n",
    "            'attention_mask':encoding['attention_mask'].squeeze(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "    \n",
    "# Khởi tạo Dataset và DataLoader cho tập huấn luyện và kiểm tra\n",
    "sarcasm_train = SarcasmTextDataset(train_data, tokenizer)\n",
    "train_loader = DataLoader(sarcasm_train, batch_size=16, shuffle=True)\n",
    "\n",
    "sarcasm_test = SarcasmTextDataset(test_data, tokenizer)\n",
    "test_loader = DataLoader(sarcasm_test, batch_size=16, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FINE TUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.3211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 1.0048\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 1.1200\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn # thư viện chứa các thành phần cho học sâu, các hàm mất mát\n",
    "from transformers import AutoModelForSequenceClassification # tải PhoBert cho nhiệm vụ phân loại\n",
    "from torch.optim import AdamW # Bộ tối ưu hóa, điều chỉnh trọng số tránh overfitting\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('vinai/phobert-base', num_labels = 4) # thêm label\n",
    "model.to('cuda' if torch.cuda.is_available() else 'cpu') # dùng GPU nếu có hoặc CPU\n",
    "\n",
    "# Set up tối ưu hóa\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5) # learning rate là 2e-5\n",
    "\n",
    "# Set up hàm mất mát\n",
    "criterion = nn.CrossEntropyLoss() # tính toán sự khác biệt label dự đoán và label thực tế\n",
    "\n",
    "# Training\n",
    "epochs = 3\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train() # dat model che do huan luyen\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Move input to device\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # zero gradients\n",
    "        optimizer.zero_grad() # xóa các gradient ở các lần tính toán trước để tránh tích lũy\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch + 1}, Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n",
      "Keyword arguments {'clean_up_tokenization_spaces': False} not recognized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5098\n",
      "Results saved to results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_results_to_json(results, phase, output_file):\n",
    "    output_data = {\n",
    "        \"results\": results,\n",
    "        \"phase\": phase\n",
    "    }\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=4)\n",
    "    print(f'Results saved to {output_file}')\n",
    "\n",
    "\n",
    "def evaluate(model, dataloader, phase):\n",
    "    model.eval()  # Đặt mô hình ở chế độ đánh giá\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    results = {}  # Khởi tạo dictionary để lưu kết quả dự đoán\n",
    "\n",
    "    with torch.no_grad():  # Tắt tính toán gradient\n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            # Di chuyển dữ liệu vào thiết bị\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # Dự đoán\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            _, preds = torch.max(outputs.logits, dim=1)  # Lấy chỉ số lớp có xác suất cao nhất\n",
    "\n",
    "            # Cộng dồn số dự đoán đúng\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)  # Cộng dồn tổng số mẫu\n",
    "\n",
    "            # Lưu kết quả dự đoán vào dictionary với ID\n",
    "            for i in range(len(preds)):\n",
    "                sample_id = batch['id'][i]  # Lấy id từ batch\n",
    "                if preds[i].item() == 0:\n",
    "                    results[sample_id] = 'non-sarcasm'\n",
    "                elif preds[i].item() == 1:\n",
    "                    results[sample_id] = 'text-sarcasm'\n",
    "                elif preds[i].item() == 2:\n",
    "                    results[sample_id] = 'image-sarcasm'\n",
    "                else:\n",
    "                    results[sample_id] = 'multi-sarcasm'\n",
    "\n",
    "    accuracy = correct / total if total > 0 else 0  # Tính độ chính xác\n",
    "    print(f'Accuracy: {accuracy:.4f}')  # In ra độ chính xác\n",
    "\n",
    "    # Lưu kết quả vào file JSON\n",
    "    save_results_to_json(results, phase, 'results.json')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "evaluate(model, test_loader, phase=\"warmup\")  # Gọi hàm evaluate để đánh giá mô hình và lưu kết quả\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
